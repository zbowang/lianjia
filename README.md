## 项目说明

使用python的scrapy爬虫框架抓取链家网二手房信息，将json格式的数据存储到本地mongodb数据库中。

经测试抓取上海7万套二手房信息没有出现bug，在代理ip限制每秒只能请求5个页面的情况下，最终耗时12小时。

如果代理限制放宽，可以减小settings.py文件中的`DOWNLOAD_DELAY`值以提高运行速度。

抓取了每个二手房详情页以及它所在小区的详情页中大多数字段。


## 启动爬虫之前

1.首先要安装好scrapy爬虫框架和配置好mongodb数据库

2.这里代理使用阿布云动态版，需要到网站上购买，再将通行证书和通行密钥配置到settings.py文件中（`PROXY_USER`和`PROXY_PASS`）


## 爬虫启动

通过在cmd中运行下面这条命令即可启动爬虫

```
scrapy crawl secondhand -a city=上海
```

支持链家网上有二手房的所有城市，只要将名称替换上面的“上海”即可



## 抓取逻辑

1.全面抓取

抓取过程中的主要问题：

以上海为例，7万二手房在一级页面只展示100页，大概3000个，且更页无法通过url规律构造。为了抓全，选择分区抓取。

而很多区仍然只显示100页，便在该区按售价划分抓取。如果还是没显示全，再继续按面积、房型划分抓取。

注：一级页面指类似这样的页面 https://sh.lianjia.com/ershoufang/pudong/pg2/

代码逻辑
- 先请求城市二手房首页，比如 https://sh.lianjia.com/ershoufang/
- 判断总页数是100还是小于100，如果有说明没有把所有房子都展示出来，就要进一步划分，在这里用`if`语句分支
- 如果总页数小于100，则抓取所有页的url，传入详情页解析函数中
- 如果总页数为100，则构造所有区一级页面的url。请求每一个区的url，对每一个再判断是否有100页，有则构造按价格划分的url……
- 如果划分到房型就直接抓取所有页url，因为几乎不可能划分到这种层次还能有100页

2.抓取小区数据

对每一个房子的详情页，获取所属小区的名称和url。因为可能很多房子属于同一个小区，为了保证每个小区只请求一次，设置一个字典存储小区的数据。每次获取一个小区都判断是否在已抓取的小区之中，如果在则直接从字典中提取信息；不再则使用小区详情页解析函数抓取该小区数据，并将其存入小区字典中。